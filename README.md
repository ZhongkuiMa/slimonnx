# **SlimONNX: Slim Your ONNX Models**

`slimonnx` is not just a tool—it's the tool to **simplify** and **optimize** your ONNX models like never before! 🔥

*Disclaimer: Unfortunately, the converted model might not be supported by ONNXRuntime anymore... But trust me, the simplicity and elegance of the optimized model are worth it!*

## Why Do We Do This? 🚀

The ONNX format is **fantastic**—it brings the power of **cross-framework interoperability** to the deep learning community. Its primary goal is simple: take your model and run it anywhere, on any platform. But let’s be real for a second... The ONNX models generated by some deep learning frameworks are **a mess**! 😱

Why? 

Because ONNX is like a "dumb" compiler—it does **zero optimizations** during the conversion. It’s a straight-up conversion from one framework to another. The result? **Inefficient models** packed with redundant operations, convoluted shapes, and unnecessary complexity.

And don’t even get me started on the **different versions** of ONNX! Sometimes attributes are inputs, sometimes they are attributes... It's chaos! 😵

That's where **SlimONNX** comes in. 💪 We take your model, simplify it, and optimize it for maximum **performance** and **readability**. You want cleaner, leaner, and faster models? We’ve got you covered.

## Features ✨

Here’s a quick rundown of what SlimONNX can do to streamline your ONNX models:

- **`constant_to_initializer`**: Convert constant nodes to initializer nodes. This is a really a thing that everyone can think of, but ONNX just doesn't do it! Actually, I know why constant node should be initializer node because initializer node is always trainable not constant node, but it really really introduce troubles. We make sure every constant is an initializer, making your model cleaner and more efficient! 💡
- **`shape_to_initializer`**: Convert shape nodes to initializers. This is a game-changer for reducing the number of nodes in your model! 🎉 We will trace the shape node and find the final node. In most of cases, the shape node is a  constant node.
- **`fuse_matmul_add`**: Fuse a MatMul and Add node into a single Gemm node. It's a standard operation in coding that ONNX just can't handle as an optimization, but we do! 🔥
- **`fuse_gemm_reshape_bn`**: Fuse a Gemm, Reshape, and BatchNormalization node into a single Gemm + Reshape node. We streamline these linear operations like never before! 💥 Because they are all linear operations.
- **`fuse_bn_reshape_gemm`**: Merge BatchNormalization, Reshape, and Gemm nodes into a unified Reshape + Gemm node. Optimization at its finest! ⚡
- **`fuse_bn_gemm`**: Combine BatchNormalization and Gemm nodes into one streamlined Gemm node. Two linear operations? We fuse them into one! ✨
- **`fuse_transpose_bn_transpose`**: Fuse Transpose, BatchNormalization, and Transpose nodes into a single Gemm node—useful for transformer-based architectures. 🔄 Because some vision data is used in the transformer structure for text data and it introduces some transpose operations.
- **`fuse_gemm_gemm`**: Combine two Gemm nodes into a single one. We donot need calculating two adjacent linear operations! 🔥
- **`fuse_conv_bn`**: Fuse Conv and BatchNormalization nodes into a single Conv node. It's the kind of optimization PyTorch already uses ([torch.nn.utils.fuse_conv_bn_eval](https://pytorch.org/docs/stable/generated/torch.nn.utils.fuse_conv_bn_eval.html)), but we take it further in the next reversed case! ⚡
- **`fuse_bn_conv`**: Merge BatchNormalization and Convolutional nodes into a single node. Not as common, but when needed, we’ve got it! 💥
- **`fuse_transposedconv_bn`**: Fuse ConvTranspose and BatchNormalization nodes into a single ConvTranspose node. Optimization made easy! 🔄
- **`shape_to_initializer`**: Convert shape nodes to initializers. Let’s get rid of unnecessary variables and treat them as initializers where possible! 🎯 This happends when the shape of a tensor is infered from another variable but the size of the variable is fixed in the model.
- **`reorder_by_strict_topological_order`**: Reorder the nodes based on strict topological order—perfect for neural network DAGs and convenient some further operations. 🏎️‘
- **`simplify_node_name`**: Simplify node names based on topological order, ditching the nested structure for clarity and simplicity. 🧠 Because ONNX names a node by the nested structure of the code.

Frankly speaking, the root reason is that ONNX is just like a reader, and it read the code without most of the optimizations. We are here to make it better! 🚀

## Installation 💻

You need Python>=3.10 and make sure these dependencies are installed ✅:

- `onnx=1.17.0`
- `numpy=1.24.3`

Maybe other versions are accepted but do not make your onnx version too old. Currently, the code consider the ONNX version 22.0.0 as a baseline.

## Current Supported Features 🌟

I have implemented most of commonly used operations in feedforward neural networks.
Transformer-based architectures will be treated as several basic operations.

## Usage 🎯

To use **SlimONNX** and turbocharge your ONNX models, simply call the `SlimONNX` class from `slimonnx/slimonnx.py`. It’s that simple!

### Example Usage 📚

There is an example about ViT model in `nets` folder ([ViT benchmark](https://github.com/ChristopherBrix/vnncomp2023_benchmarks/tree/main/benchmarks/vit/onnx) from [VNNCOMP'23](https://sites.google.com/view/vnn2023/home)).

```python
from slimonnx import SlimONNX
import onnx

if __name__ == "__main__":
    slimonnx = SlimONNX()
    onnx_path = "../nets/ibp_3_3_8.onnx"

    # Convert the model to version 22 to avoid many inconsistencies
    model = onnx.load(onnx_path)
    model = onnx.version_converter.convert_version(model, target_version=22)
    onnx_path = onnx_path.replace(".onnx", "_v22.onnx")
    onnx.save(model, onnx_path)

    target_path = onnx_path.replace(".onnx", "_simplified.onnx")

    slimonnx.slim(
        onnx_path,
        target_path,
        constant_to_initializer=True,
        shape_to_initializer=True,
        fuse_matmul_add=True,
        fuse_transpose_bn_transpose=True,
        fuse_gemm_gemm=True,
        fuse_bn_gemm=True,
        reorder_by_strict_topological_order=True,
        simplify_node_name=True,
        verbose=True,
    )
```

[netron.app](netron.app) is a good way to check the computation graph of the onnx file.
You can clearly see the difference between the original and the optimized model.

### See the Difference! 

You can see the difference below!
The left is the original onnx model and the right is the optimized one.

![example_comparison_vit](./README.assets/example_comparison_vit.png)

