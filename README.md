# **SlimONNX: Slim Your ONNX Models! ⚡**

`slimonnx` is not just a tool—it's the tool to **simplify** and **optimize** your ONNX models like never before! 🔥

*Disclaimer: Unfortunately, the converted model might not be supported by ONNXRuntime anymore... But trust me, the simplicity and elegance of the optimized model are worth it!*

## Why Do We Do This? 🚀

The ONNX format is **fantastic**—it brings the power of **cross-framework interoperability** to the deep learning community. Its primary goal is simple: take your model and run it anywhere, on any platform. But let’s be real for a second... The ONNX models generated by some deep learning frameworks are **a mess**! 😱

Why? 

Because ONNX is like a "dumb" compiler—it does **zero optimizations** during the conversion. It’s a straight-up conversion from one framework to another. The result? **Inefficient models** packed with redundant operations, convoluted shapes, and unnecessary complexity.

And don’t even get me started on the **different versions** of ONNX! Sometimes attributes are inputs, sometimes they are attributes... It's chaos! 😵

That's where **SlimONNX** comes in. 💪 We take your model, simplify it, and optimize it for maximum **performance** and **readability**. You want cleaner, leaner, and faster models? We’ve got you covered.

## Features ✨

Here’s a quick rundown of what SlimONNX can do to streamline your ONNX models:

- **`fuse_matmul_add`**: Fuse a MatMul and Add node into a single Gemm node. It's a standard operation in coding that ONNX just can't handle as an optimization, but we do! 🔥
- **`fuse_gemm_reshape_bn`**: Fuse a Gemm, Reshape, and BatchNormalization node into a single Gemm + Reshape node. We streamline these linear operations like never before! 💥 Because they are all linear operations.
- **`fuse_bn_reshape_gemm`**: Merge BatchNormalization, Reshape, and Gemm nodes into a unified Reshape + Gemm node. Optimization at its finest! ⚡
- **`fuse_bn_gemm`**: Combine BatchNormalization and Gemm nodes into one streamlined Gemm node. Two linear operations? We fuse them into one! ✨
- **`fuse_transpose_bn_transpose`**: Fuse Transpose, BatchNormalization, and Transpose nodes into a single Gemm node—useful for transformer-based architectures. 🔄 Because some vision data is used in the transformer structure for text data and it introduces some transpose operations.
- **`fuse_gemm_gemm`**: Combine two Gemm nodes into a single one. We donot need calculating two adjacent linear operations! 🔥
- **`fuse_conv_bn`**: Fuse Conv and BatchNormalization nodes into a single Conv node. It's the kind of optimization PyTorch already uses ([torch.nn.utils.fuse_conv_bn_eval](https://pytorch.org/docs/stable/generated/torch.nn.utils.fuse_conv_bn_eval.html)), but we take it further in the next reversed case! ⚡
- **`fuse_bn_conv`**: Merge BatchNormalization and Convolutional nodes into a single node. Not as common, but when needed, we’ve got it! 💥
- **`fuse_transposedconv_bn`**: Fuse ConvTranspose and BatchNormalization nodes into a single ConvTranspose node. Optimization made easy! 🔄
- **`shape_to_initializer`**: Convert shape nodes to initializers. Let’s get rid of unnecessary variables and treat them as initializers where possible! 🎯 This happends when the shape of a tensor is infered from another variable but the size of the variable is fixed in the model.
- **`simplify_node_name`**: Simplify node names based on topological order, ditching the nested structure for clarity and simplicity. 🧠 Because ONNX names a node by the nested structure of the code.
- **`reorder_by_strict_topological_order`**: Reorder the nodes based on strict topological order—perfect for neural network DAGs and convenient some further operations 🏎️

Frankly speaking, the root reason is that ONNX is just like a reader, and it read the code without most of the optimizations. We are here to make it better! 🚀

## Installation 💻

Make sure these dependencies are installed:

- `onnx`
- `numpy`

## Usage 🎯

To use **SlimONNX** and turbocharge your ONNX models, simply call the `SlimONNX` class from `slimonnx/slimonnx.py`. It’s that simple!

Here’s an example:

```python
from slimonnx.slimonnx import SlimONNX

# Initialize SlimONNX
slim = SlimONNX()

# Simplify your ONNX model
slim.slim("model.onnx", "slim_model.onnx")
```
