# 🌟 **SlimONNX: *Slim* Your ONNX Models**

**slimonnx** is not just a tool—it's *the* tool to **simplify** and **optimize** your ONNX models like never before! 🔥

> ⚠️ *Disclaimer:* The converted model **might not** be supported by ONNXRuntime anymore...  
> But trust me — the simplicity and elegance of the optimized model are totally worth it!  
> It’s mainly designed as a preprocessing tool for **neural network verification**, where we need to manually inspect layers and build a verification model. A well-simplified model offers a solid starting point, even if ONNXRuntime can't run it anymore.

To complete this repo, one important task is to infer the shape of the ONNX model, and we can infer and simplify some constant nodes. For this, please refer to another interesting repo [ShapeONNX](https://github.com/ZhongkuiMa/shapeonnx).

## 🚀 Why Do We Do This?

ONNX is awesome — it allows **cross-framework** model deployment. Its mission is simple: take your model and run it *anywhere*. But let's be honest for a moment...

The ONNX models generated by some frameworks are **a hot mess** 😱. Why?

Because ONNX is like a *"dumb"* compiler — it does **zero** optimization during export.  
The result? Inefficient models bloated with:

- Redundant operations 🪵
- Convoluted shapes 🧩
- Unnecessary complexity 🌀

And don’t even get me started on ONNX version inconsistencies — attributes are sometimes inputs, sometimes not... it’s chaos! 😵

### That’s where **SlimONNX** comes in. 💪

We take your model and simplify it for maximum:

- ⚡ Performance
- 🧼 Readability
- 🧠 Clarity

You want cleaner, leaner, and faster models? We've got you covered.

## ✨ Features

**SlimONNX** is developer- and researcher-friendly because it’s:

- 🐍 **Pure Python:** No C/C++ dependencies. Easy to set up!
- 💡 **Lightweight:** Only depends on `onnx` and `numpy`. No heavy frameworks.
- 📖 **Simple:** Easy-to-read code. Want to add your own optimization pass? Go for it!
- ⚡ **Fast:** Thanks to efficient code design — minimal heavy computation.
- 🔄 **Generalized:** Works across ONNX versions. Framework-agnostic. Flexible.

### 🌟 Currently Supported Optimizations

I have implemented most of commonly used operations in feedforward neural networks. Transformer-based architectures will be treated as several basic operations.

Here’s a quick rundown of what SlimONNX can do to streamline your ONNX models:

- **constant_to_initializer**: Convert constant nodes to initializer nodes. This is a really a thing that everyone can think of, but ONNX just doesn't do it! Actually, I know why constant node should be initializer node because initializer node is always trainable not constant node, but it really really introduce troubles. We make sure every constant is an initializer, making your model cleaner and more efficient! 💡
- **shape_to_initializer**: Convert shape nodes to initializers. This is a game-changer for reducing the number of nodes in your model! 🎉 We will trace the shape node and find the final node. In most of cases, the shape node is a constant node.
- **fuse_matmul_add**: Fuse a MatMul and Add node into a single Gemm node. It's a standard operation in coding that ONNX just can't handle as an optimization, but we do! 🔥
- **fuse_gemm_reshape_bn**: Fuse a Gemm, Reshape, and BatchNormalization node into a single Gemm + Reshape node. We streamline these linear operations like never before! 💥 Because they are all linear operations.
- **fuse_bn_reshape_gemm**: Merge BatchNormalization, Reshape, and Gemm nodes into a unified Reshape + Gemm node. Optimization at its finest! ⚡
- **fuse_bn_gemm**: Combine BatchNormalization and Gemm nodes into one streamlined Gemm node. Two linear operations? We fuse them into one! ✨
- **fuse_transpose_bn_transpose**: Fuse Transpose, BatchNormalization, and Transpose nodes into a single Gemm node—useful for transformer-based architectures. 🔄 Because some vision data is used in the transformer structure for text data, and it introduces some transpose operations.
- **fuse_gemm_gemm**: Combine two Gemm nodes into a single one. We donot need calculating two adjacent linear operations! 🔥
- **fuse_conv_bn**: Fuse Conv and BatchNormalization nodes into a single Conv node. It's the kind of optimization PyTorch already uses ([torch.nn.utils.fuse_conv_bn_eval](https://pytorch.org/docs/stable/generated/torch.nn.utils.fuse_conv_bn_eval.html)), but we take it further in the next reversed case! ⚡
- **fuse_bn_conv**: Merge BatchNormalization and Convolutional nodes into a single node. Not as common, but when needed, we’ve got it! 💥
- **fuse_transposedconv_bn**: Fuse ConvTranspose and BatchNormalization nodes into a single ConvTranspose node. Optimization made easy! 🔄
- **shape_to_initializer**: Convert shape nodes to initializers. Let’s get rid of unnecessary variables and treat them as initializers where possible! 🎯 This happends when the shape of a tensor is infered from another variable but the size of the variable is fixed in the model.
- **remove_redundant_reshape**: Remove redundant Reshape nodes. This happens when we use [onnx.version_converter](https://onnx.ai/onnx/api/version_converter.html) sometimes. I dnk why... There is an example in current test folder.
- **remove_redundant_operations**: This aims to remove some operation that does not make affect to the final output. For example, adding or subtracting a zero constant. It *indeed* happens in some models...
- **simplify_node_name**: Simplify node names based on topological order, ditching the nested structure for clarity and simplicity. 🧠 Because ONNX names a node by the nested structure of the code.
- **reorder_by_strict_topological_order**: Reorder the nodes based on strict topological order—perfect for neural network DAGs and convenient some further operations. 🏎️‘

Frankly speaking, the root reason is that ONNX is just like a reader, and it read the code without most of the optimizations. We are here to make it better! 🚀

## 🎯 Usage

To use **SlimONNX** and turbocharge your ONNX models, simply call the `SlimONNX` class from `slimonnx/slimonnx.py`. It’s that simple!

### 💻 Installation

You need Python>=3.10 (we are using Python 3.12) and make sure these dependencies are installed ✅:

- `onnx=1.17.0`
- `numpy=2.2.4`

Maybe other versions are accepted but do not make your onnx version too old. Currently, the code consider the ONNX version 22.0.0 as a baseline.

> We recommend you use [onnx.version_converter](https://onnx.ai/onnx/api/version_converter.html) to convert the model to version 22.0.0 as an input model of SlimONNX to support the operations and this is the version that we are using in the test.

### 📚 Example Usage

#### Test Examples of VNNCOMP'24

You need to get the repo of [vnncomp2024](https://github.com/ChristopherBrix/vnncomp2024_benchmarks). This repo does not contain the benchmarks folder because it is about 20GB. The testing examples are in the `test_vnncomp` folder. Then you make sure the following folder structure:

```
- slimonnx
    ├── slimonnx
    ├── README.md
    ├── test_vnncomp
    ├── ...
- vnncomp2024
    ├── benchmarks
    ├── ...
```

#### 😋 An Example of ViT Model

There is an example about ViT model in `nets` folder ([ViT benchmark](https://github.com/ChristopherBrix/vnncomp2023_benchmarks/tree/main/benchmarks/vit/onnx) from [VNNCOMP'23](https://sites.google.com/view/vnn2023/home)).

```python
from slimonnx import SlimONNX
import onnx

if __name__ == "__main__":
    slimonnx = SlimONNX()
    onnx_path = "../nets/ibp_3_3_8.onnx"

    # Convert the factory to version 22 to avoid many inconsistencies
    model = onnx.load(onnx_path)
    model = onnx.version_converter.convert_version(model, target_version=22)
    onnx_path = onnx_path.replace(".onnx", "_v22.onnx")
    onnx.save(model, onnx_path)

    target_path = onnx_path.replace(".onnx", "_simplified.onnx")

    slimonnx.slim(
        onnx_path,
        target_path,
        constant_to_initializer=True,
        shape_to_initializer=True,
        fuse_matmul_add=True,
        fuse_transpose_bn_transpose=True,
        fuse_gemm_gemm=True,
        fuse_bn_gemm=True,
        reorder_by_strict_topological_order=True,
        simplify_node_name=True,
        verbose=True,
    )
```

[netron.app](netron.app) is a good way to check the computation graph of the onnx file. You can clearly see the difference between the original and the optimized model.

### 😀 See the Difference!

You can see the difference below!
The left is the original onnx model and the right is the optimized one.

![example_comparison_vit](./README.assets/example_comparison_vit.png)

## 🤝 Contributing

We warmly welcome contributions from everyone! Whether it's fixing bugs 🐞, adding features ✨, improving documentation 📚, or just sharing ideas 💡—your input is appreciated!

📌 NOTE: Direct pushes to the `main` branch are restricted. Make sure to fork the repository and submit a Pull Request for any changes!
